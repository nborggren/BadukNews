{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2 as ul\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import re\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def HanParse(pageno,write=0):\n",
    "    newspage = ul.urlopen('http://baduk.hangame.com/news.nhn?gseq='+str(pageno)+'&m=view&page=1&searchfield=&leagueseq=0&searchtext=')\n",
    "    soup = bs(newspage,'html.parser')\n",
    "    #[s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
    "    tmp = soup.getText()\n",
    "    tmp=u'내용'.join(tmp.split(u'내용')[2:])\n",
    "    b=tmp.find(u'관련 뉴스보기')\n",
    "    tmp=tmp[:b]\n",
    "    tmp='\\n'.join([i for i in tmp.split('\\n') if len(i)>0])\n",
    "    if write==1:\n",
    "        h=codecs.open('./han/'+str(pageno)+'.d','w',encoding='utf-8')\n",
    "        h.write(tmp)\n",
    "    return tmp\n",
    "\n",
    "def TyParse(pageno,write=0):\n",
    "    newspage = ul.urlopen('http://www.tygem.com/news/news/viewpage.asp?seq='+str(pageno)+'&gubun=&igubun=&find=&findword=')\n",
    "    soup = bs(newspage,'html.parser')\n",
    "    [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
    "    tmp = soup.getText()\n",
    "    a=tmp.find(u'Home > 뉴스        Daily뉴스')\n",
    "    b=tmp.find(u'TYGEM / ')\n",
    "    tmp = [i for i in tmp[a:b].split('\\n') if len(i)>0]\n",
    "    vtext = '\\n'.join(tmp[1:])\n",
    "    if write==1:\n",
    "        h=codecs.open('./dat/'+str(pageno)+'.d','w',encoding='utf-8')\n",
    "        h.write(vtext)\n",
    "    return vtext\n",
    "\n",
    "def OroParse(pageno,write=0):\n",
    "    newspage = ul.urlopen('http://www.cyberoro.com/news/news_view.oro?div_no=A1&num='+str(pageno)+'&pageNo=1&cmt_n=0')\n",
    "    soup = bs(newspage,'html.parser')\n",
    "    [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
    "    tmp = soup.getText()\n",
    "    a=tmp.find(u'Home > 뉴스')\n",
    "    b=tmp.find(u'┃관련뉴스')\n",
    "    tmp = tmp[a:b]\n",
    "    tmp = [i for i in tmp.split('\\n')[1:] if len(i)>0]\n",
    "    vis = '\\n'.join(tmp)\n",
    "    if write==1:\n",
    "        h=codecs.open('./oro/'+str(pageno)+'.d','w',encoding='utf-8')\n",
    "        h.write(vis)\n",
    "    return vis\n",
    "\n",
    "def GetSent(pageno):\n",
    "    dat=HanParse(pageno)\n",
    "    sent = dat.split('.')[6:-1]\n",
    "    first = sent[0].split(u'내용')\n",
    "    sent[0]=first[-1]\n",
    "    return sent[:-3]\n",
    "\n",
    "def ReadSent(pageno,src='./han/'):\n",
    "    dat=codecs.open(src+str(pageno)+'.d',encoding='utf-8')\n",
    "    dat = dat.read()\n",
    "    dat = Clean(dat)\n",
    "    dat = dat.replace('?','.')\n",
    "    dat = dat.replace('!','.')\n",
    "    return [' '.join(i.split()) for i in dat.split('.')]\n",
    "    \n",
    "def Clean(sentence,comma=0):\n",
    "    if comma==0:\n",
    "        for i in ['\\n','_','-','(',')','\"','\\'',u'▲','...','[',']',u'■','<','>','\\r']:\n",
    "            sentence=sentence.replace(i,' ')\n",
    "    else:\n",
    "        for i in ['\\n','_','-','(',')','\"','\\'',u'▲','...','[',']',u'■','<','>',',','\\r']:\n",
    "            sentence=sentence.replace(i,' ')\n",
    "    return sentence.strip()\n",
    "\n",
    "def GetHanArticles(indexno):\n",
    "    articles=[]\n",
    "    main=ul.urlopen('http://baduk.hangame.com/news.nhn?&page='+str(indexno))\n",
    "    mainbs=bs(main,'html.parser')\n",
    "    tmp = [link.get('href') for link in mainbs.find_all('a')]\n",
    "    for j in tmp:\n",
    "        try:\n",
    "            if j.find('readnews')>0:\n",
    "                articles.append(j)\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    mynews=[int(re.search(r'\\d+', i).group()) for i in articles]\n",
    "    mynews = sorted(list(set(mynews)))\n",
    "    return mynews\n",
    "\n",
    "def GetTyArticles(indexno):\n",
    "    articles=[]\n",
    "    main=ul.urlopen('http://www.tygem.com/news/news/list.asp?pagec='+str(indexno))\n",
    "    mainbs=bs(main,'html.parser')\n",
    "    tmp = [link.get('href') for link in mainbs.find_all('a')]\n",
    "    for j in tmp:\n",
    "        try:\n",
    "            if j.find('/news/news/view.asp?gubun=&seq=')>-1:\n",
    "                articles.append(j)\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    mynews=[int(re.search(r'\\d+', i).group()) for i in articles]\n",
    "    mynews = sorted(list(set(mynews)))\n",
    "    return mynews\n",
    "\n",
    "def GetOroArticles(pageno):\n",
    "    articles=[]\n",
    "    loc=\"http://cyberoro.com/news/news_list.oro?cmt_n=0&div_no=A1&pageNo=\"+str(pageno)+\"&blockNo=1\"\n",
    "    newspage = ul.urlopen(loc)\n",
    "    soup = bs(newspage,'html.parser')\n",
    "    tmp = [link.get('href') for link in soup.find_all('a')]\n",
    "    for j in tmp:\n",
    "        try:\n",
    "            if j.find('news_view.oro?pageNo=')>0:\n",
    "                articles.append(j)\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    articles = [i[i.find('num'):] for i in articles]\n",
    "    mynews=[int(re.search(r'\\d+', i).group()) for i in articles]\n",
    "    return mynews\n",
    "\n",
    "def WordCount(pagelist,sort=1,sentence=0):\n",
    "    sentences=[]\n",
    "    words=[]\n",
    "    for page in pagelist:\n",
    "        \n",
    "        tmp=GetSent(page)\n",
    "        for sent in tmp:\n",
    "            sentences.append(Clean(sent))\n",
    "    for sent in sentences:\n",
    "        for j in sent.split():\n",
    "            words.append(j)\n",
    "    uwords = list(set(words))\n",
    "    wc = [(i,words.count(i)) for i in uwords]\n",
    "    if sort==1:\n",
    "        wc = sorted(wc, key=lambda x: x[1])\n",
    "        wc.reverse()\n",
    "    if sentence==0:\n",
    "        return wc\n",
    "    else:\n",
    "        return wc, sentence\n",
    "\n",
    "#a la https://github.com/mouuff/Google-Translate-API/blob/master/python/GoogleTranslate.py\n",
    "def translate(to_translate, to_langage=\"auto\", langage=\"auto\"):\n",
    "    to_translate=to_translate.encode('utf-8')\n",
    "    agents = {'User-Agent':\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET CLR 3.0.04506.30)\"}\n",
    "    before_trans = 'class=\"t0\">'\n",
    "    link = \"http://translate.google.com/m?hl=%s&sl=%s&q=%s\"% (to_langage, langage, to_translate.replace(\" \", \"+\"))\n",
    "    request = ul.Request(link, headers=agents)\n",
    "    page = ul.urlopen(request).read()\n",
    "    result = page[page.find(before_trans)+len(before_trans):]\n",
    "    result = result.split(\"<\")[0]\n",
    "    return result\n",
    "\n",
    "def UpdateData(src='han'):\n",
    "    if src=='han':\n",
    "        f=open('./lists/han.list')\n",
    "        files = [int(line.split('.')[0]) for line in f]\n",
    "        cnt=1\n",
    "        articles=GetHanArticles(1)\n",
    "        while min(articles) not in files:\n",
    "            cnt+=1\n",
    "            articles+=GetHanArticles(cnt)\n",
    "        articles=[j for j in articles if j not in files]\n",
    "        print 'updating ', len(articles), ' hangame files'    \n",
    "        for i in articles:HanParse(i,write=1)\n",
    "        return articles\n",
    "    \n",
    "    if src=='ty':\n",
    "        f=open('./lists/dat.list')\n",
    "        files = [int(line.split('.')[0]) for line in f]\n",
    "        cnt=1\n",
    "        articles=GetTyArticles(1)\n",
    "        while min(articles) not in files:\n",
    "            cnt+=1\n",
    "            articles+=GetTyArticles(cnt)\n",
    "        articles=[j for j in articles if j not in files]\n",
    "        print 'updating ', len(articles), ' tygem files'    \n",
    "        for i in articles:TyParse(i,write=1)\n",
    "        return articles\n",
    "\n",
    "    if src=='oro':\n",
    "        f=open('./lists/oro.list')\n",
    "        files = [int(line.split('.')[0]) for line in f]\n",
    "        articles=GetOroArticles(1)\n",
    "        cnt=1\n",
    "        checks = [i in files for i in articles]\n",
    "        while checks.count(True)==0 :\n",
    "            cnt+=1\n",
    "            articles+=GetTyArticles(cnt)\n",
    "            checks = [i in files for i in articles]\n",
    "            print cnt\n",
    "        articles=[j for j in articles if j not in files]\n",
    "        print 'updating ', len(articles), ' oro files'    \n",
    "        for i in articles:TyParse(i,write=1)\n",
    "        return articles\n",
    "    \n",
    "    if src=='all':\n",
    "        articles=UpdateData(src='han')\n",
    "        articles+=UpdateData(src='ty')\n",
    "        articles+=UpdateData(src='oro')\n",
    "        return articles\n",
    "\n",
    "def MergeWords():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating  8  hangame files\n",
      "updating  9  tygem files\n",
      "updating  14  oro files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[35520,\n",
       " 35521,\n",
       " 35525,\n",
       " 35528,\n",
       " 35535,\n",
       " 35538,\n",
       " 35545,\n",
       " 35562,\n",
       " 20640,\n",
       " 20641,\n",
       " 20657,\n",
       " 20658,\n",
       " 20659,\n",
       " 20660,\n",
       " 20661,\n",
       " 20662,\n",
       " 20663,\n",
       " 521280,\n",
       " 521279,\n",
       " 521278,\n",
       " 521277,\n",
       " 521240,\n",
       " 521276,\n",
       " 521239,\n",
       " 521275,\n",
       " 521274,\n",
       " 521273,\n",
       " 521225,\n",
       " 521272,\n",
       " 521271,\n",
       " 521269]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UpdateData(src='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20639, 20640, 20641, 20657, 20658, 20659, 20660, 20661, 20662, 20663]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetTyArticles(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35520, 35521, 35525, 35528, 35535, 35538, 35545, 35562]\n"
     ]
    }
   ],
   "source": [
    "print articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles+=GetArticles(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35520,\n",
       " 35521,\n",
       " 35525,\n",
       " 35528,\n",
       " 35535,\n",
       " 35538,\n",
       " 35545,\n",
       " 35562,\n",
       " 35501,\n",
       " 35504,\n",
       " 35506,\n",
       " 35507,\n",
       " 35510,\n",
       " 35517,\n",
       " 35518,\n",
       " 35519]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35501"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
